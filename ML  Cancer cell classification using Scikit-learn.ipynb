{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b59774",
   "metadata": {},
   "source": [
    "# Step #1: Importing the necessary module and dataset.We will be needing the ‘Scikit-learn’ module and the Breast cancer wisconsin (diagnostic) dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7933810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Python module\n",
    "import sklearn\n",
    " \n",
    "# importing the dataset\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f40f54",
   "metadata": {},
   "source": [
    "# Step #2: Loading the dataset to a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07efd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "data = load_breast_cancer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca19b55",
   "metadata": {},
   "source": [
    "# Step #3: Organizing the data and looking at it. To get a better understanding of what the dataset contains and how we can use the data to train our model, let us first organize the data and then see what it contains by using the print() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a2211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize our data\n",
    "label_names = data['target_names']\n",
    "labels = data['target']\n",
    "feature_names = data['feature_names']\n",
    "features = data['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95046329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# looking at the data #Then, using the print() function, let us examine the data. \n",
    "print(label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa4857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c20ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49101ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ccb06",
   "metadata": {},
   "source": [
    "# Step #4: Organizing the data into Sets. For testing the accuracy of our classifier, we must test the model on unseen data. \n",
    "# So, before building the model, we will split our data into two sets, viz., training set and test set.\n",
    "# We will be using the training set to train and evaluate the model and then use the trained model to make predictions on the unseen test set.\n",
    "# The sklearn module has a built-in function called the train_test_split(), which automatically divides the data into these sets. We will be using this function to split the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66bc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data\n",
    "train, test, train_labels, test_labels = train_test_split(features, labels,\n",
    "                                         test_size = 0.33, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c52d2",
   "metadata": {},
   "source": [
    "# Step #5: Building the Model.\n",
    "# There are many machine learning models to choose from. All of them have their own advantages and disadvantages. \n",
    "# For this model, we will be using the Naive Bayes algorithm that usually performs well in binary classification tasks.\n",
    "# Firstly, import the GaussianNB module and initialize it using the GaussianNB() function. Then train the model by fitting it to the data in the dataset using the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff143527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module of the machine learning model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initializing the classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# training the classifier\n",
    "model = gnb.fit(train, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59526da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# making the predictions\n",
    "predictions = gnb.predict(test)\n",
    "\n",
    "# printing the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ec21a",
   "metadata": {},
   "source": [
    "# Step #6: Evaluating the trained model’s accuracy.\n",
    "# As we have predicted values now, we can evaluate our model’s accuracy by comparing it with the actual labels of the test set, i.e., comparing predictions with test_labels.\n",
    "# For this purpose, we will be using the built-in accuracy_score() function in the sklearn module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415241c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9414893617021277\n"
     ]
    }
   ],
   "source": [
    "# importing the accuracy measuring function\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# evaluating the accuracy\n",
    "print(accuracy_score(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe162c89",
   "metadata": {},
   "source": [
    "# So, we find out that this machine learning classifier based on the Naive Bayes algorithm is 94.15% accurate in predicting whether a tumor is malignant or benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05593628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
